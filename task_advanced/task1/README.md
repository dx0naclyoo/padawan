# **Анализ большого текстового файла**

**Описание**:  
Программа анализирует большой текстовый файл, обрабатывая его по частям (батчам), чтобы не загружать весь файл в память. В результате выводится:  
- Общее количество слов  
- Количество уникальных слов  
- Топ-10 самых частых слов  

**Зачем**:  
Позволяет анализировать огромные текстовые файлы (например, книги, логи, дампы), не расходуя всю оперативную память.  

**Пример вывода**:  
```bash
Общее количество слов: 125678  
Количество уникальных слов: 25689  

Топ-10 самых частых слов:  
и: 5678 раз  
в: 4321 раз  
до: 3456 раз  
...  
```


**Подсказка**:
<details>
<summary>Раскрыть подсказку</summary>

Для реализации используются:
Построчное чтение файла с обработкой батчами
```python
with open('file.txt', 'r') as file:
    file.readline() # читаем только 1 линию
    # Тут стоит заметить, что чтение начинается всегда с самого начала файла, это требует ещё кода, чтобы мы не читали 1 и то же миллион раз
    file.seek(cursor) # cursor - int, указатель, откуда начанается чтение файла
    cursor = file.tell() # узнать где находится курсор, cursor: int
```

</details>